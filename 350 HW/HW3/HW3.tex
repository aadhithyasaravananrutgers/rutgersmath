\documentclass{exam}
\usepackage{amsmath}

\pagestyle{headandfoot}
\firstpageheadrule
\runningheadrule
\firstpageheader{Prof. Tahvildar-Zadeh \\ Linear Algebra}{Homework 3}{Aadhithya Saravanan}
\runningheader{Linear Algebra \\ Homework 3}{}{Saravanan}
\firstpagefooter{}{}{}
\runningfooter{ }{\thepage}{ }

\printanswers

\begin{document}

\underline{Exercise 1.3.23}
\newline
\begin{parts}
    \part $W_1+W_2=\{x+y \mid x \in W_1, y \in W_2\}$. To show that $W_1+W_2$ is a subspace of $V$, we need to show that it is closed under addition and scalar multiplication, and that it contains the zero vector. For any $u \in W_1+W_2$ and $v \in W_1+W_2$, we can write $u=x_1+y_1$ and $v=x_2+y_2$ where $x_1,x_2 \in W_1$ and $y_1,y_2 \in W_2$. Then $u+v=(x_1+y_1)+(x_2+y_2)=(x_1+x_2)+(y_1+y_2)$, which is in $W_1+W_2$. For any scalar $c$, we have $cu = c(x+y) = cx + cy$, which is in $W_1+W_2$. Finally, the zero vector is in $W_1+W_2$ because it can be written as the sum of the zero vectors from each subspace. To show that $W_1$ is in $W_1+W_2$, we can write any vector $w \in W_1$ as $w + 0$, where $0$ is the zero vector in $W_2$. Then, this $w+0$ is in $W_1+W_2$. Similarly, any vector in $W_2$ can be written as $0 + w$, where $w$ is a vector in $W_2$. Then, this $0+w$ is in $W_1+W_2$. Therefore, both $W_1$ and $W_2$ are contained in $W_1+W_2$.
    \part We can assume $V$ contains $W_1$ and $W_2$ as subspaces. Then, we can write any vector $v \in W_1+W_2$ as $v=w_1+w_2$, where $w_1 \in W_1$ and $w_2 \in W_2$. Since $W_1$ and $W_2$ are subspaces of $V$, we have that $w_1$ and $w_2$ are also in $V$. Therefore, the sum of any two vectors in $W_1+W_2$ is also in $V$, which means that $W_1+W_2$ is contained in $V$.
    \newline
\end{parts}

\underline{Exercise 1.3.25}
\newline
\newline
For $P(F)=W_1\oplus W_2$, we need to show that $W_1 \cap W_2 = \{0\}$ and $W_1+W_2=P(F)$. First, we will show that $W_1 \cap W_2 = \{0\}$. $W_1$ is the set of odd degree polynomials, and $W_2$ is the set of even degree polynomials. The only polynomial that is both odd and even is the zero polynomial, which means that $W_1 \cap W_2 = \{0\}$. Next, we will show that $W_1+W_2=P(F)$. Any polynomial in $P(F)$ can be written as the sum of an odd degree polynomial and an even degree polynomial. For example, if we have a polynomial $p(x) = a_nx^n + a_{n-1}x^{n-1} + ... + a_1x + a_0$, we can write it as $p(x) = (a_nx^n + a_{n-2}x^{n-2} + ... ) + (a_{n-1}x^{n-1} + a_{n-3}x^{n-3} + ... )$, where the first part is an odd degree polynomial and the second part is an even degree polynomial. Therefore, any polynomial in $P(F)$ can be expressed as the sum of an odd degree polynomial and an even degree polynomial, which means that $W_1+W_2=P(F)$. Thus, we have shown that $P(F)=W_1\oplus W_2$.
\newline

\underline{Exercise 1.3.28}
\newline
\newline
The set $W_1$ of all skew-symmetric $n \times n$ matrices with entries from $F$ is a subspace of $M_{n \times n}(F)$. To show that $W_1$ is a subspace, we need to show that it is closed under addition and scalar multiplication, and that it contains the zero vector. For any $A, B \in W_1$, we have $A^T = -A$ and $B^T = -B$. Then, $(A+B)^T = A^T + B^T = -A - B = -(A+B)$, which means that $A+B$ is also in $W_1$. For any scalar $c$, we have $(cA)^T = cA^T = c(-A) = -(cA)$, which means that $cA$ is also in $W_1$. Finally, the zero matrix is in $W_1$ because it is skew-symmetric. Therefore, $W_1$ is a subspace of $M_{n \times n}(F)$. Now we let $W_2$ be the subspace of $M_{n \times n}(F)$ consisting of all symmetric matrices. We will show that $M_{n \times n}(F) = W_1 \oplus W_2$. First, we will show that $W_1 \cap W_2 = \{0\}$. The only matrix that is both skew-symmetric and symmetric is the zero matrix, which means that $W_1 \cap W_2 = \{0\}$. Next, we will show that $W_1 + W_2 = M_{n \times n}(F)$. Any matrix $A$ in $M_{n \times n}(F)$ can be written as the sum of a skew-symmetric matrix and a symmetric matrix. For example, we can write $A$ as $A = \frac{1}{2}(A - A^T) + \frac{1}{2}(A + A^T)$, where $\frac{1}{2}(A - A^T)$ is a skew-symmetric matrix and $\frac{1}{2}(A + A^T)$ is a symmetric matrix. Therefore, any matrix in $M_{n \times n}(F)$ can be expressed as the sum of a skew-symmetric matrix and a symmetric matrix, which means that $W_1 + W_2 = M_{n \times n}(F)$. Thus, we have shown that $M_{n \times n}(F) = W_1 \oplus W_2$.
\newline

\underline{Exercise 1.4.14}
\newline
\newline
To show that $\operatorname{span}(S_1 \cup S_2) = \operatorname{span}(S_1) + \operatorname{span}(S_2)$, we need to show that both sides are subsets of each other. First, we will show that $\operatorname{span}(S_1 \cup S_2) \subseteq \operatorname{span}(S_1) + \operatorname{span}(S_2)$. Any vector in $\operatorname{span}(S_1 \cup S_2)$ can be written as a linear combination of vectors from $S_1$ and $S_2$. We can separate this linear combination into two parts: one part that is a linear combination of vectors from $S_1$ and another part that is a linear combination of vectors from $S_2$. Therefore, any vector in $\operatorname{span}(S_1 \cup S_2)$ can be expressed as the sum of a vector from $\operatorname{span}(S_1)$ and a vector from $\operatorname{span}(S_2)$, which means that it is in $\operatorname{span}(S_1) + \operatorname{span}(S_2)$. To show the other inclusion, we will show that $\operatorname{span}(S_1) + \operatorname{span}(S_2) \subseteq \operatorname{span}(S_1 \cup S_2)$. Any vector in $\operatorname{span}(S_1) + \operatorname{span}(S_2)$ can be written as the sum of a vector from $\operatorname{span}(S_1)$ and a vector from $\operatorname{span}(S_2)$. Since each vector in $\operatorname{span}(S_1)$ is a linear combination of vectors from $S_1$, and each vector in $\operatorname{span}(S_2)$ is a linear combination of vectors from $S_2$, the sum of these vectors is a linear combination of vectors from $S_1 \cup S_2$. Therefore, any vector in $\operatorname{span}(S_1) + \operatorname{span}(S_2)$ is in $\operatorname{span}(S_1 \cup S_2)$. Thus, we have shown that $\operatorname{span}(S_1 \cup S_2) = \operatorname{span}(S_1) + \operatorname{span}(S_2)$.
\newline

\underline{Exercise 1.5.17}
\newline
\newline
The columns of a square upper triangular matrix with nonzero diagonal entries are linearly independent. Let $A$ be an $n \times n$ matrix with such properties. The $i$-th column of $A$ has a nonzero entry in the $i$-th row and zeros below it. This means that the $i$-th column cannot be written as a linear combination of the previous columns, which have zeros in the $i$-th row. Therefore, each column of $A$ is linearly independent from the others, and thus the columns of $A$ are linearly independent.
\newline

\underline{Exercise 1.6.15}
\newline
\newline
A basis for $W$ is the set of matrices $B=D\cup U$. $D$ is the set of matrices with zero entries everywhere except for the $i$-th and $i+1$-th diagonal entry, which are two scalars of opposite signs, in which $i$ ranges from $1$ to $n-1$. $U$ is the set of matrices with zero entries everywhere except for any entry not on the diagonal, where it is any scalar. In this way, $D$ allows us to construct matrices with a trace of 0, and $U$ allows us to construct matrices entries not on the diagonal. To find the dimension of $W$, we need to count the number of matrices in the basis. The set $D$ has $n-1$ matrices, and the set $U$ has $n^2-n$ matrices (since there are $n$ entries on the diagonal in an $n \times n$ matrix). Therefore, the total number of matrices in the basis is $(n-1)+n^2-n=n^2-1$. Thus, the dimension of $W$ is $n^2-1$.
\newline

\underline{Exercise 1.6.17}
\newline
\newline
A basis for $W$ is the set of matrices with zero entries everywhere except for the $(i,j)$-th entry and the $(j,i)$-th entry, which are two scalars of opposite signs, where $i$ and $j$ range from $1$ to $n$ and $i \neq j$. In this way, we can construct any skew-symmetric matrix by choosing appropriate scalars for the entries. To find the dimension of $W$, we need to count the number of matrices in the basis. For each pair of indices $(i,j)$ with $i < j$, we have one matrix in the basis. There are $\binom{n}{2}$ such pairs, which means that there are $\binom{n}{2}$ matrices in the basis. Therefore, the dimension of $W$ is $\binom{n}{2} = \frac{n(n-1)}{2}$.
\newline

\underline{Exercise 1.6.25}
\newline
\newline
If $Z$ is composed of all possible linear combinations of the vectors $V$ and $W$, then a basis of $Z$ would be comprised of the vectors in $V$ and $W$ that are linearly independent. For $V$, a basis would be the set of vectors $\{v_1, v_2, \dots, v_m\}$, which has $m$ vectors. For $W$, a basis would be the set of vectors $\{w_1, w_2, \dots, w_n\}$, which has $n$ vectors. To find a basis for $Z$, we can use $(v_i,0)$ and $(0,w_j)$ for each $i$ and $j$, where $v_i \in V$ and $w_j \in W$. This gives us a total of $m+n$ vectors in the basis. Therefore, the dimension of $Z$ is $m+n$.
\newline

\underline{Exercise 1.6.27}
\newline
\newline
The dimension of the subspace $W_1\cap P_n(F)$ is the number of vectors in the basis of $W_1\cap P_n(F)$. A basis for $W_1\cap P_n(F)$ can be found by taking the basis of $W_1$ and restricting it to polynomials of degree at most $n$. Since $W_1$ is the subspace of all polynomials only odd degree terms, a basis for $W_1$ is $\{x, x^3, \dots\}$. When we restrict this basis to polynomials of degree at most $n$, we get the basis $\{x, x^3, \dots, x^n\}$ for odd $n$ and $\{x, x^3, \dots, x^{n-1}\}$ for even $n$. Therefore, the dimension of $W_1\cap P_n(F)$ is $\lceil \frac{n}{2} \rceil$.
\newline
\newline
The dimension of the subspace $W_2\cap P_n(F)$ is the number of vectors in the basis of $W_2\cap P_n(F)$. A basis for $W_2\cap P_n(F)$ can be found by taking the basis of $W_2$ and restricting it to polynomials of degree at most $n$. Since $W_2$ is the subspace of all polynomials only even degree terms, a basis for $W_2$ is $\{1, x^2, x^4, \dots\}$. When we restrict this basis to polynomials of degree at most $n$, we get the basis $\{1, x^2, x^4, \dots, x^n\}$ for even $n$ and $\{x^2, x^4, \dots, x^{n-1}\}$ for odd $n$. Therefore, the dimension of $W_2\cap P_n(F)$ is $\lfloor \frac{n}{2} \rfloor+1$.
\newline

\underline{Exercise 1.6.30}
\newline
\newline
$W_1$ is a subspace if it is closed under addition and scalar multiplication, and contains the zero vector. For any $u, v \in W_1$, we have $u=\begin{bmatrix}a_1, b_1 \\ c_1, a_1\end{bmatrix}$ and $v=\begin{bmatrix}a_2, b_2 \\ c_2, a_2\end{bmatrix}$. So, their sum is $u+v=\begin{bmatrix}a_1+a_2, b_1+b_2 \\ c_1+c_2, a_1+a_2\end{bmatrix}$, which is in $W_1$. For any scalar $k$, we have $ku=\begin{bmatrix}ka_1, kb_1 \\ kc_1, ka_1\end{bmatrix}$, which is also in $W_1$. Finally, the zero vector $\begin{bmatrix}0, 0 \\ 0, 0\end{bmatrix}$ is in $W_1$. Therefore, $W_1$ is a subspace. Now we will show that $W_2$ is a subspace. For any $u, v \in W_2$, we have $u=\begin{bmatrix}0, a_1 \\ -a_1, b_1\end{bmatrix}$ and $v=\begin{bmatrix}0, a_2 \\ -a_2, b_2\end{bmatrix}$. So, their sum is $u+v=\begin{bmatrix}0, a_1+a_2 \\ -(a_1+a_2), b_1+b_2\end{bmatrix}$, which is in $W_2$. For any scalar $k$, we have $ku=\begin{bmatrix}0, ka_1 \\ -ka_1, kb_1\end{bmatrix}$, which is also in $W_2$. Finally, the zero vector $\begin{bmatrix}0, 0 \\ 0, 0\end{bmatrix}$ is in $W_2$. Therefore, $W_2$ is a subspace. The dimension of $W_1$ is 3, since a basis for $W_1$ is $\left\{\begin{bmatrix}1, 0 \\ 0, 1\end{bmatrix}, \begin{bmatrix}0, 1 \\ 0, 0\end{bmatrix}, \begin{bmatrix}0, 0 \\ 1, 0\end{bmatrix}\right\}$. The dimension of $W_2$ is 2, since a basis for $W_2$ is $\left\{\begin{bmatrix}0, 1 \\ -1, 0\end{bmatrix}, \begin{bmatrix}0, 0 \\ 0, 1\end{bmatrix}\right\}$. The dimension of $W_1 + W_2$ is 4, since a basis for $W_1 + W_2$ is $\left\{\begin{bmatrix}1, 0 \\ 0, 0\end{bmatrix}, \begin{bmatrix}0, 1 \\ 0, 0\end{bmatrix}, \begin{bmatrix}0, 0 \\ 1, 0\end{bmatrix}, \begin{bmatrix}0, 0 \\ 0, 1\end{bmatrix}\right\}$. The top left and bottom right entries are $W_1$ and $W_2$ allow for the top left and bottom right entries of $W_1+W_2$ to be any scalar, and the top right and bottom left entries of $W_1$ allow for the top right and bottom left entries of $W_1+W_2$ to be any scalar. Therefore, the dimension of $W_1 + W_2$ is 4. The dimension of $W_1 \cap W_2$ is 1, since a basis for $W_1 \cap W_2$ is $\left\{\begin{bmatrix}0, 1 \\ -1, 0\end{bmatrix}\right\}$. The top left entry of $W_2$ forces the top left entry of $W_1 \cap W_2$ to be 0, and the bottom right entry of $W_1$ being the same as the top left entry of $W_1$ forces the bottom right entry of $W_1 \cap W_2$ to be 0. The top right and bottom left entries of $W_1$ force the top right and bottom left entries of $W_1 \cap W_2$ to be opposite scalars. Therefore, the dimension of $W_1 \cap W_2$ is 1.
\end{document}